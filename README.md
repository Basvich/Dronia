# Dronia üñãÔ∏è

This project was generated with [Angular CLI](https://github.com/angular/angular-cli) version 15.2.1. Actualizado a versi√≥n 16

El objetivo principal de este proyecto es probar el aprendizaje reforzado en un entorno (pensado b√°sicamente para programadores). Algunas de las cosas para probar, aprender (como programador):

* Usar tensorFlow JS correctamente, junto con typescript y todo lo que significa para usar los tipos correctos de variables (tensores).
* Entender bien que estamos haciendo cuando estamos usando aprendizaje reforzado. Que significa exactamente lo de estados finitos, y como se aproximan recompensas. Ver que cosas pueden funcionar o no.
* Usar Three Js, para poder tener una forma de realimentaci√≥n visual simple y poder tener gr√°ficos y animaciones que permita ver que ocurre con lo que estamos simulando.
* Indirectamente ir probando/introduciendo buenas pr√°cticas para programar en las √∫ltimas versiones de angular.

> ‚ö†Ô∏è Esta misma wiki, se usa como repositorio de notas que se van tomando, no como un documento final, repasado, corregido, etc. con lo que es mas que probable que tenga ideas repetidas, no tan bien redactadas...En alg√∫n futuro de vez en cuando se ir√° repasando la documentaci√≥n para dejarla mejor.



## Ejemplos


![Alt text](wiki/media/pantallazo1.png)

### Entrenamiento simple en 1D



En este caso, lo que se busca es partiendo de un espacio continuo, con las variables posici√≥n y velocidad, se quiere realizar un entrenamiento reforzado para que el dron se mantenga estable a cierta altura.
En la red, tenemos 2 entradas, y 5 posibles estados de salida, correspondientes a las posibles acciones a tomar: 5 posibles niveles de potencia.

Para controlar el dron, se obtiene el array de las 5 posibles acciones, y se toma la que tenga la recompensa mayor.

El objetivo de la red, es entonces acabar entrenando el campo recompensa de cada uno de los posibles estados.

üëâ Ampliar esto en [mas informaci√≥n detallada](./wiki/aproximacionPrimera.md)

### Entrenamiento simple en 1D version 2

Este caso es una variaci√≥n del simple, pero en vez de manejar fuerzas deseadas, lo que se maneja es la variaci√≥n de fuerzas, o sea el control b√°sicamente indica si queremos m√°s o menos fuerza aplicada. Entre las variables de entrada tambi√©n se incluye la fuerza sentida por el dron (cuando est√° en reposo, ser√≠a de 1 apuntando hacia abajo)

üëâ Ampliar esto en [Entrenamiento 1D 2¬™ versi√≥n](./wiki/aproximacionPrimeraB.md)

### Entrenamiento con movimiento en 2D

Se permite que el movimiento sea en 2D, con lo que tenemos control de fuerza y orientaci√≥n.

üëâ Ampliar esto en [Entrenamiento 2D](./wiki/aproximacion2D.md)

#### Jurado para las recompensas

Para valorar la recompensa asignada a un estado, se cre√≥ una clase llamada *TReward*. La forma actual para calcular la recompensa, es calcular una posici√≥n proyectada, que consiste en la posici√≥n actual, a la que se le a√±ade la velocidad. Con esta posici√≥n proyectada, entonces se obtiene el valor recompensa de forma lineal y mayor cuanto m√°s cerca se encuentre de la altura objetivo.

#### Problemas encontrados

Es muy importante que los estados iniciales del dron, sean aleatorios tanto en posici√≥n como en velocidad, para permitir ir rellenando adecuadamente las posibles recompensas esperadas para cada estado diferente.

La red converge bastante r√°pidamente hacia el estado que considera mejor, el cual es mantener una fuerza neutra en el dron. Esto se ve en que en los datos obtenidos por la red, se nota la recompensa para el estado neutro, con bastante mas valor que las dem√°s.

Una vez que el dron tiende a mantenerse dentro de los limites pero simplemente quieto, casi sin tendencia a acercase a la altura objetivo, al ir realizando m√°s entrenamientos, se nota como esperado que las recompensas de los dem√°s estados van creciendo tambi√©n, pero ese proceso es tremendamente largo hasta que el dron aprenda a moverse de esas posiciones de estabilidad hacia la posici√≥n deseada.

# Ruta

* Entrenar la red para movimiento en 3D en el espacio
* Manipulaci√≥n de la red neuronal
  * Persistencia
  * Comprobar si una red entrenada, se puede reusar para otra red ligeramente diferente, y supone alguna ventaja.
* A√±adir algoritmos geneticos para mejorar algunas caracteristicas como precisi√≥n del aprendizaje, rapidez u optimizaci√≥n de los movimientos para conseguir algo, que simplemente usando recomensas de individuo simple son m√°s dificiles de modelizar.
* A√±adir otra red que permita al dron indicar cual es a su vez el target deseado, por ejemplo para poder "planificar" rutas o evitar obstaculos.



# Readme por defecto

Teniendo el entorno con node.js instalado (>=18), y angular (actualmente la v16), se puede descargar el proyecto, y usando visual code, ejecutar directamente sin ninguna otra dependencia especial, ya que est√°n todas incluidas dentro (como cualquier proyecto angular).

### Development server

Run `ng serve` for a dev server. Navigate to `http://localhost:4200/`. The application will automatically reload if you change any of the source files.

### Code scaffolding

Run `ng generate component component-name` to generate a new component. You can also use `ng generate directive|pipe|service|class|guard|interface|enum|module`.

### Build

Run `ng build` to build the project. The build artifacts will be stored in the `dist/` directory.

### Running unit tests

Run `ng test` to execute the unit tests via [Karma](https://karma-runner.github.io).

### Running end-to-end tests

Run `ng e2e` to execute the end-to-end tests via a platform of your choice. To use this command, you need to first add a package that implements end-to-end testing capabilities.

### Further help

To get more help on the Angular CLI use `ng help` or go check out the [Angular CLI Overview and Command Reference](https://angular.io/cli) page.
