# Primera aproximaci贸n

Solo se usa en 1 Dimension (vertical), para poder probar distintas las distintas partes que se necesitan, y poder tener un buen control y visualizaci贸n de los datos para no entrar en muchas variables que har铆an dificil el an谩lisis.

En la primera aproximaci贸n para resolver el problema, se parte de un sistema con estados continuos como entrada y con un conjunto de estados finito de salida.
Como datos de la entrada, se usa la diferencia en altura con la altura deseada y la velocidad (vertical)

## Estrategia general

Se usa apredizage reforzado, partiendo de estados continuos y queriendo obtener posibles acciones (finitas en la primera versi贸n), con una predicci贸n de recompensa para cada una de ellas, que permita escoger la acci贸n que m谩s recompensa d谩 para controlar el dron.

En un ciclo de entrenamiento, se realizan 2 partes diferenciadas:

* Ciclo de simulaci贸n, en el que en cada paso, con el estado del dron (altura, velocidad), se ejecuta la predicci贸n de la red, y se toma la acci贸n mas favorable. Con el estado nuevo obtenido, se calcula una recompensa, y se guarda en una memoria temporal (solo los n 煤ltimos), el estado del dron, que acci贸n se tom贸, junto con la recompensa obtenida.
* Ciclo de aprendizaje, en el que se parte de los datos almacenados, y se pasan a la red para su aprendizaje.

En el ciclo de aprendizaje, se tiene una cierta probabilidad peque帽a de tomar otro camino, diferente al de mayor recompensa, lo que permite que se puedan explorar otras v铆as.

En el ciclo de aprendizaje, se usa una variaci贸n de la ecuacion de  en la que la recompensa que se usa para el aprendizaje es la suma ponderada entre la recompensa del paso, y la maxima recompensa esperada de los futuros casos.

> Al acabar un ciclo de simulaci贸n, dependiendo si se acab贸 por tiempo, por salirse de limites o simplemente conseguir el objetivo, se otorga una recompensa adicional, la cual se reparte hac铆a atr谩s 

> 锔 Es importante remarcar, que solo se modifica la recompensa correspondiente a la acci贸n que se prob贸, las demas acciones siguen con el valor que ten铆an, lo que puede reforzar comportamientos no optimos.

## Acciones posibles

Las posibles acciones que se quieren tener en cuenta para saber cual es la siguiente acci贸n, son 5 niveles posibles de potencia (incluyendo el neutro=peso). Se pens贸 en usar 5 en vez de simplmente 3 para ver si con mas niveles de potencia se pod铆a obtener un ajuste mas fino del control, 

## Recompensas

Para la recompensa, como tiene que ver con la posici贸n y velocidad que tiene en un momento dado, una forma simple, es en un momento, sumamos la posici贸n con la velocidad (escalada), de forma que b谩sicamente se puntua donde estar谩 el vehiculo en el paso siguiente. Con esta posici贸n, simplemente se usa una funci贸n cuadr谩tica con la distancia para obtener un valor dado de recompensa.
 
Si el drone se encuentra en un peque帽o intervalo de la altura deseada y con una velocidad muy peque帽a, entonces se a帽ade a la recompensa otro valor por conseguir el objectivo.

Si se sale de los l铆mites, se a帽ade una penalizaci贸n.

## Pruebas del concepto

### Gr谩ficas resultado

En todas los siguientes ejemplos, cada gr谩fica corrresponde con un una fuerza aplicada: Fueza 0, 0.5, 1... y as铆 a lo que corresponden con los 5 posibles estados de salida obtenidos de la red.
En el eje vertical, tenemos para cada punto la recompensa esperada si se toma esa opci贸n. En el eje horizontal, tenemos la altura (recordad que por defecto 6 es el target esperado). Cada conjunto de gr谩ficas, a su vez se entienden que es para una velocidad vertical del dron.

Como estamos reproduciendo las recompensas esperadas, entonces en un estado cualquiera, el dron tomar谩 la acci贸n que le proporcione la mayor recompensa, o sea la que est谩 en la parte superior.



<img align="left" src="media/graph_1_a.png" alt="Velocidad 0">

En esta gr谩fica (tomada despu茅s de unos 60 ciclos de entrenamiento), podemos ver que acci贸n tomar谩 el dron seg煤n su altura y velocidad inicial 0.
* Si lo posicionamos en la posici贸n 1, vemos que la gr谩fica que est谩 por arriba es F2 (el doble de fuerza que el peso, lo que hace que suba), la cual da una recompensa esperada sobre 5. 
* Si lo posicionamos en la posici贸n 11, se ve que la acci贸n que tomar谩 es la de usar una potencia baja, lo que hace que el dron baje de posici贸n

---

<img align="left" src="media/graph_1_b.png" alt="Velocidad -a">
En est谩 otra gr谩fica, la velocidad inicial es -1 (hacia abajo).
* Hasta la altura 9, se ve que la acci贸n preferida es usar la potencia 2. Lo que hace que si est谩 por arriba del 6, vaya frenando, y por debajo directamente suba.
* En posiciones un poco mas altas, la fuerza es 1.5, que viene a ser _frenar pero no tanto_.

---


<img align="left" src="media/graph_1_c.png" alt="Velocidad 0.5">
Con una ligera velocidad positiva de 0.5, la acci贸n preferida es directamente tener poca potencia: F0.5, lo que hace que el dron tienda a bajar.

Finalmente cuando se est谩 probando la simulaci贸n, lo que ocurre es que el dron va cambiando de estado (equivalente a cambiar en cada momento de gr谩fica), y lo que ocurre es que va saltando constantemente entre una potencia seleccionada y otra, lo que hace que acabe posicionado en el entorno de la altura deseada de 6.
