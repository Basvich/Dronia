# Primera aproximaci칩n

Solo se usa en 1 Dimension (vertical), para poder probar distintas las distintas partes que se necesitan, y poder tener un buen control y visualizaci칩n de los datos para no entrar en muchas variables que har칤an dificil el an치lisis.

En la primera aproximaci칩n para resolver el problema, se parte de un sistema con estados continuos como entrada y con un conjunto de estados finito de salida.
Como datos de la entrada, se usa la diferencia en altura con la altura deseada y la velocidad (vertical)

## Estrategia general

Se usa apredizage reforzado, partiendo de estados continuos y queriendo obtener posibles acciones (finitas en la primera versi칩n), con una predicci칩n de recompensa para cada una de ellas, que permita escoger la acci칩n que m치s recompensa d치 para controlar el dron.

En un ciclo de entrenamiento, se realizan 2 partes diferenciadas:

* Ciclo de simulaci칩n, en el que en cada paso, con el estado del dron (altura, velocidad), se ejecuta la predicci칩n de la red, y se toma la acci칩n mas favorable. Con el estado nuevo obtenido, se calcula una recompensa, y se guarda en una memoria temporal (solo los n 칰ltimos), el estado del dron, que acci칩n se tom칩, junto con la recompensa obtenida.
* Ciclo de aprendizaje, en el que se parte de los datos almacenados, y se pasan a la red para su aprendizaje.

En el ciclo de aprendizaje, se tiene una cierta probabilidad peque침a de tomar otro camino, diferente al de mayor recompensa, lo que permite que se puedan explorar otras v칤as.

En el ciclo de aprendizaje, se usa una variaci칩n de la ecuacion de 游뱂游뱂 en la que la recompensa que se usa para el aprendizaje es la suma ponderada entre la recompensa del paso, y la maxima recompensa esperada de los futuros casos.

> 游눠Al acabar un ciclo de simulaci칩n, dependiendo si se acab칩 por tiempo, por salirse de limites o simplemente conseguir el objetivo, se otorga una recompensa adicional, la cual se reparte hac칤a atr치s 

> 丘멆잺 Es importante remarcar, que solo se modifica la recompensa correspondiente a la acci칩n que se prob칩, las demas acciones siguen con el valor que ten칤an, lo que puede reforzar comportamientos no optimos.

## Acciones posibles

Las posibles acciones que se quieren tener en cuenta para saber cual es la siguiente acci칩n, son 5 niveles posibles de potencia (incluyendo el neutro=peso). Se pens칩 en usar 5 en vez de simplmente 3 para ver si con mas niveles de potencia se pod칤a obtener un ajuste mas fino del control, 

## Recompensas

Para la recompensa, como tiene que ver con la posici칩n y velocidad que tiene en un momento dado, una forma simple, es en un momento, sumamos la posici칩n con la velocidad (escalada), de forma que b치sicamente se puntua donde estar치 el vehiculo en el paso siguiente. Con esta posici칩n, simplemente se usa una funci칩n cuadr치tica con la distancia para obtener un valor dado de recompensa.
 
Si el drone se encuentra en un peque침o intervalo de la altura deseada y con una velocidad muy peque침a, entonces se a침ade a la recompensa otro valor por conseguir el objectivo.

Si se sale de los l칤mites, se a침ade una penalizaci칩n.

## Pruebas del concepto

### Gr치ficas resultado

En todas los siguientes ejemplos, cada gr치fica corrresponde con un una fuerza aplicada: Fueza 0, 0.5, 1... y as칤 a lo que corresponden con los 5 posibles estados de salida obtenidos de la red.
En el eje vertical, tenemos para cada punto la recompensa esperada si se toma esa opci칩n. En el eje horizontal, tenemos la altura (recordad que por defecto 6 es el target esperado). Cada conjunto de gr치ficas, a su vez se entienden que es para una velocidad vertical del dron.

Como estamos reproduciendo las recompensas esperadas, entonces en un estado cualquiera, el dron tomar치 la acci칩n que le proporcione la mayor recompensa, o sea la que est치 en la parte superior.

<img  src="media/graph_1_a.png" alt="Velocidad 0">

En esta gr치fica (tomada despu칠s de unos 60 ciclos de entrenamiento), podemos ver que acci칩n tomar치 el dron seg칰n su altura y velocidad inicial 0.
* Si lo posicionamos en la posici칩n 1, vemos que la gr치fica que est치 por arriba es F2 (el doble de fuerza que el peso, lo que hace que suba), la cual da una recompensa esperada sobre 5. 
* Si lo posicionamos en la posici칩n 11, se ve que la acci칩n que tomar치 es la de usar una potencia baja, lo que hace que el dron baje de posici칩n

---

<img  src="media/graph_1_b.png" alt="Velocidad -a">
En est치 otra gr치fica, la velocidad inicial es -1 (hacia abajo).
* Hasta la altura 9, se ve que la acci칩n preferida es usar la potencia 2. Lo que hace que si est치 por arriba del 6, vaya frenando, y por debajo directamente suba.
* En posiciones un poco mas altas, la fuerza es 1.5, que viene a ser _frenar pero no tanto_.

---
<img  src="media/graph_1_c.png" alt="Velocidad 0.5">
Con una ligera velocidad positiva de 0.5, la acci칩n preferida es directamente tener poca potencia: F0.5, lo que hace que el dron tienda a bajar.

Finalmente cuando se est치 probando la simulaci칩n, lo que ocurre es que el dron va cambiando de estado (equivalente a cambiar en cada momento de gr치fica), y lo que ocurre es que va saltando constantemente entre una potencia seleccionada y otra, lo que hace que acabe posicionado en el entorno de la altura deseada de 6.

---

### Otras pruebas para probar el resultado.

Cuando se est치 simulando el modelo, se puede a침adir una carga adicional (payload), la cual se suma a la masa del dron. Hay que tener en cuenta que en ningun momento esta masa adicional forma parte del entrenamiento, y se usa para ver que ocurre con un modelo entrenado.

Lo que ocurre al a침adir una masa es que la posici칩n de equilibrio del dron, en vez de estar en el entorno de la altura deseada, esta se situa por debajo. Esto es coherente si pensamos en que realmente con este modelo es como si un operador aprende a volar un dron memorizando que posiciones del control de potencia hay que usar en cada situaci칩n. O sea que si est치 en tal posici칩n y velocidad, pues se mueve hasta tal posici칩n. Si esta por debajo, sabe que tiene que poner tal potencia, pero si esa potencia no es superior que la necesaria para compensar la masa adicional, pues simplemente no subir치. Una operador normal, lo que har칤a ser칤a simplemente aumentar algo m치s la potencia para subir.
